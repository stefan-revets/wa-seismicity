\documentclass{report}
\usepackage{chicago,booktabs,SIUnits}

\title{Northwest Shelf Seismicity and statistics}
\author{Stefan A. Revets}
\date{}

\begin{document}
\maketitle
\chapter{Introduction}

Probabilistic Seismic Hazard Analysis (PSHA) relies fundamentally on
earthquake statistics and on rock and soil mechanics. Earthquake
statistics provide the magnitude-recurrence relations. Rock and soil
mechanics provide the ground motion response to a given earthquake.
Measurements, correlations and modelling rely almost exclusively on
events in tectonically active regions, most notably California
\cite{gutenberg-richter44:frequency,knopoff_al82:b-values,kaklamanos_al10:implementation}.
In contrast, the earthquakes occurring on the Australian continent are
a prime example of intra-plate seismicity. The processes and drivers
of intra-plate seismicity are and remain enigmatic. However, it is
clear that these processes are very different from those driving the
tectonic earthquakes \cite{stein07:approaches}. Ground motion
predictions are empirically derived equations (for an overview, see
\citeNP{kaklamanos_al10:implementation}), based on data collected over
rock and over siliciclastic sediments, largely from observations in
the continental Americas. There have been some attempts to redress
this situation for Australian soils, but the models are commonly
similar to the American ones
(\cite{lam_wilson08:new,leonard-al07:model}). No implementations have
been developed specifically for calcareous sediments.  A number of
separate but linked investigations are reported upon here with the aim
to help and address some of the shortcomings currently plaguing PSHA
on the Northwest Shelf.

\chapter{Data Sources}
\section{Earthquake data} 
The earthquake data used in this study are taken from the publicly
accessible Geoscience Australia earthquake catalogue (available
on-line from www.ga.gov.au). The catalogue lists the timing, location,
and magnitude of earthquakes in Australia going back to 1888. The
vexed issue of completeness has been discussed by
\citeNP{leonard08:hundred} and also
\citeNP{sagar_leonard07:mapping}. These authors also discuss issues
encountered with the value of magnitude as recorded over the years,
and in particular changes to the scales and the way in which these
changes have been applied.  A subset of this catalogue was defined to
include events which have occurred in an area encompassing the
Northwest Shelf (16\degree -– 23\degree\ S, 112\degree -– 124\degree\ E;
see Figure 1). The magnitudes have been corrected as discussed below.

\section{Earthquake waveforms}
A substantial dataset of broadband seismometer recordings (from
October 2005 to October 2008) from ARC Linkage Project LP0560955 was
also available \cite{revets-al09:intraplate}. The continuous and
simultaneous recording by 8 instruments placed on a variety of
undergrounds (calcareous sediments, siliciclastic sediments, and
Archean bedrock) provided an opportunity of investigating differential
ground responses for Australian soils and sediments.

\chapter{Methodology}
\section{Probabilistic Seismic Hazard Analysis}

\citeNP{cornell68:engineering} approach. The mean annual frequency of exceeding
of a fixed acceleration level f(a) at a given site is given by:
\begin{equation}
  \label{eq:cornell}
f(a) = \sum r_\mathrm{source} \int\int F_{\mathrm{PGA} \geq a/m}(r) f_M(m) f_R(r) dm dr
\end{equation}
where $r_\mathrm{source}$ is the annual activity rate, $F_{\mathrm{PGA
    \geq a/m}}(r)$ the indicator function for the predicted Peak Ground
Acceleration of an earthquake of magnitude $m$ at a distance $r$ from
the site with respect to the acceleration level $a$, and with $f_M(m)$
and $f_R(r)$ the probability density functions of magnitude M and
distance R for the source considered.

\section{Statistics of Magnitude}

One of the important regularities found to govern the complex
process of seimic activity is a relationship between size and
frequency of earthquakes. The Gutenberg-Richter equation states
that 
\begin{equation}
  \label{eq:gutenberg-richter}
N_T(M) = 10^{a - b M} 
\end{equation}
where $N$ is the number of events with magnitude larger than $M$ over
a sufficiently long period of time $T$
\cite{gutenberg-richter44:frequency}. While this equation has been
widely and successfully applied, it suffers from limitations.
Gutenberg-Richter plots always show a more or less developed
``shoulder'' towards the lower end of the magnitude axis, partly due
to under reporting of lower magnitude earthquakes. Towards the higher
end of the magnitude axis, the scatter of the data points increases
considerably, due to the paucity of the largest earthquakes. In many
instances, a systematic convexity of the magnitude-frequency line can
be seen. These general observations provide a warning that the
statistics of a regression analysis will be suspect.  A further
significant limitation is that the relationship shows no (upper)
limit: it allows extrapolations to be made to earthquakes with an
(arbitrarily) large magnitude. The finite amount of energy stored in
the Earth, and the finite strength of rocks dictate that there must be
an upper limit to the magnitude of an earthquake. More or less
significant deviations from the Gutenberg-Richter formula have been
documented for the largest magnitudes (e.g.,
\citeNP{pisarenko_sornette04:rigorous}). Two modifications (``corner''
magnitude and maximum magnitude) have been proposed and are often
adopted. Their shortcomings have led to the adaptation and adoption of
the more appropriate, statistically sound methods from the study of
extremal events (\cite{embrechts_al97:extremal}).

\subsection{“Corner” magnitude}

\citeNP{kagan97:seismic,kagan_schoenberg01:estimation} and
\citeNP{vere-jones_etal01:remarks} multiplied the power law distribution
of the seismic moments (which corresponds to the Gutenberg-Richter
distribution of magnitudes) by an exponential taper. This results in
either a $\Gamma$ or a modified Pareto distribution, with a characteristic
moment, or ``corner'' magnitude equivalent in the corresponding
magnitude distribution. The effect is a ``soft'' truncation of the
Gutenberg-Richter law.

\subsection{Maximum magnitude}

The proposal to set a maximum possible earthquake size $M_\mathrm{max}$, a
hard truncation of the Gutenberg-Richter law
\cite{consentino_al77:truncated,dargahi-noubary83:procedure,main_al99:constraints}
plays an important role in seismic risk and seismic hazard studies
\cite{bender_perkins93:_treatment,cornell94:statistical,kijiko_graham98:parametric}. It
provides a very attractive measure for engineers and insurers alike,
as risk and construction standards can be set against a given maximum
magnitude.  Unfortunately, the estimation or calculation of $M_{max}$
remains unsatisfactory. Its superficial attraction is undermined by
undesirable features
\cite{kagan93:statistics,pisarenko_etal08:new_approach}:
\begin{enumerate}
\item $M_\mathrm{max}$ is ill-defined, as it does not contain the time scale over
  which it has been determined, or over which is valid.
\item The cut-off nature of $M_\mathrm{max}$ is arbitrary in the sense that
  the impossibility of $M_\mathrm{max} + \epsilon$ for any arbitrarily small
  value of $\epsilon$ has no (physical) justification.
\item $M_\mathrm{max}$ is statistically highly unstable.
\end{enumerate}

\subsection{Extreme Value Theory}

Extreme value theory provides the theory to handle extremes of random
phenomena. In contrast to basic statistics, it provides the necessary
tools to deal with statistical distribution issues such as skewness,
fat tails, rare events and the like \cite{embrechts_al97:extremal}.
\citeNP{pisarenko_etal08:characterisation} applied with success the
Generalised Extreme Value distribution (GEV) and the Generalised
Pareto distribution (GPD) in their attempts to characterise the
distribution of earthquake magnitudes and go beyond the limitations of
the Gutenberg-Richter relation and its various ad-hoc modifications.
The Frechet-Fisher-Tipper theorem \cite{embrechts_al97:extremal} leads
to the definition of the GEV distribution
\begin{equation}
  \label{eq:GEV}
\Phi(x|\mu,\sigma,\xi) = e^{- (1 + \xi (x - \mu)/ \sigma)^{-1/\xi)}},  
\end{equation}
a Pareto ($ξ>0$) or a Weibull ($ξ<0$) distribution which
degenerates for $\xi = 0$ to 
\begin{equation}
  \label{eq:gumbel}
\Phi(x|\mu,\sigma) = e^{-e^{-(x - \mu)/ \sigma}},  
\end{equation}
(Gumbel distribution) in which $\mu$, $\sigma$, $\xi$ are respectively
the centering, scale and shape parameters. The theorem provides the
statistical and mathematical justification for this definition of the
limiting distribution of the maxima of identically independently
distributed random variables x as the sample size n goes to infinity.
The quantiles $Q_q$ of the GEV distribution $\Phi(x|\mu,\sigma,\xi)$ are
\begin{equation}
  \label{eq:quantiles}
Q_q(\tau) = \mu(T) + ((\tau/T \log(1/q))^{\xi} - 1) \sigma(T)/\xi  
\end{equation}
with $q$ the quantile confidence level, $\tau$ an arbitrary time
interval (in the future) and $T$ the time interval step used to
estimate the parameters of the GEV distribution.  The utility and
value of the quantiles over $M_\mathrm{max}$ as stable and statistically
meaningful values was demonstrated and illustrated by
\citeNP{pisarenko_etal08:characterisation,pisarenko_etal08:new_approach}.
In practice, the parameters of the GEV distribution can be estimated
from data through maximum likelihood calculations. A set of scripts
and routines were implemented in R (R Development Core Team, 2011),
and are listed in Appendix A1.

\section{Statistics of Recurrence Rates}

The analysis and calculation of recurrence rates or waiting times of
seismic events is beset with difficulties equivalent to those
encountered in the study of magnitude distribution. It is clear that
the obvious extension of the Gutenberg-Richter magnitude-number
relation to a magnitude-frequency relation by simply dividing the
numbers by the time interval, compounds the statistical problems just
discussed by bringing in explicitly the time dimension. Size, number
and timing of seismic events are related, but the nature of the
relationships is complex and not fully understood.  A radically
different approach was proposed by \citeNP{bak_al02:unified}, and
further developed and refined by
\citeNP{corral05:renormalisation,corral06:dependence,corral09:statistical}. Their
application of the theories of self-organised criticality led to a
modified $\Gamma$ distribution, so that
\begin{equation}
  \label{eq:mod_gamma}
  p(x) = C \delta x^{\gamma-1} e^{-(x/\alpha)^{\delta}} / \alpha^\gamma \Gamma(\gamma/\delta)   
\end{equation}
where $x = \lambda t$ (the average seismic rate multiplied by time).

\citeNP{saichev_sornette06:universal} criticised this analysis as
incompatible and at variance with the data and they promoted the
Epidemic-Type Aftershock Sequence as a better approach.  The
Epidemic-Type Aftershock Sequence (ETAS) was proposed by
\citeNP{kagan_knopoff81:stochastic} and \citeNP{ogata88:statistical}, with
the statistical and mathematical properties studied and expanded
further over the years (see \citeNP{saichev_sornette07:theory} for an
overview). It integrates the Gutenberg-Richter relationship with the
Omori relationship of aftershock sequences, and adds a productivity
law and a measure of the fractal nature of fault networks.
\begin{equation}
  \label{eq:etas}
p(x) = (\alpha n \theta \rho^\theta x^{-1-\theta} + (1 - n + \alpha n \rho^\theta x^{-\theta})^2) e^{( -(1 - n) x - \alpha n \theta \rho^\theta x^{1-\theta}/(1-\theta))}
\end{equation}
where $x = \lambda t$ (the average seismic rate multiplied by time),
$\alpha = (\lambda_0 c)^\theta$ (reflecting the Omori aftershock law),
$\rho = \lambda/ \lambda_0 = Q(m) (L/L_0)^d$ (a measure of the
productivity law), and with $n$ the criticality parameter. The function
accounts much better for the observations, as can be expected from a
4-parameter function.  The parameters of the function can be fitted to
the data through maximum likelihood calculations. A set of scripts and
routines were implemented in R (R Development Core Team, 2011), and
are listed in Appendix A2.

\section{Attenuation Models}

\citeNP{kaklamanos_al10:implementation} NGA approach. It incorporates
various ``standard'' ground motion models, which may be quite
inappropriate for the calcareous sediments on the Northwest Shelf. It
is not yet clear to me if it is possible to get a more accurate
reflection by using more appropriate input parameters in those models.

\section{Boundary Conditions}

Assess to what extent (statistical) assumptions are fulfilled by
the data.

\chapter{Calculations}
\section{Magnitudes and Timing}
\subsection{Catalogue Magnitude}

\citeNP{sagar_leonard07:mapping} discussed the magnitude determination
and the way this has changed over the years. They illustrated these
changes with a figure of the pre- and post-1992 Western Australian
earthquake magnitude Cumulative Density graph, including an equivalent
graph with the South Australian earthquakes. The Geoscience Australia
earthquakes catalogue is herein re-analysed and the observations
presented by Sagar \& Leonard are reconfirmed (see Figure 2).  The
catalogue of events used herein has the magnitude values for all
pre-1992 events corrected by subtracting 0.5 from the originally
listed values.

\subsection{Stationary, Poissonian, process}

The statistical techniques applied in this study assume a
stationary Poisson process. A plot of all events in the catalogue
against time shows immediately that the process is not
stationary (see Figure 3). A stationary period appears to be
present between May 1979 and November 2002 (for a total of 485
events): a plot of events against time yields the expected, and
required, linear relation (see Figure 4).

\section{Ground Attenuation}

\chapter{Results}
\section{Seismic recurrence and magnitudes}

De-clustering algorithm removes very few events (17 out of 485)
as aftershocks Sensitivity tests of magnitude error: perturbing
the catalogue magnitudes with Gaussian errors (mean 0.0, standard
deviation 0.2) has no significant effect on the estimates. If
anything, it improves the convergence and stability of parameter
estimation. The bootstrapping procedure can be declared
successful.  The 0.975 quantile of earthquake magnitude to be
expected in the next 5,000 Yr is 6.5 

\chapter{Discussion}

\bibliography{biblio_seismology}
\bibliographystyle{chicago}

\end{document}
