#+TITLE: Seismicity Statistics through Extreme Value Theory

* Introduction
Calculate the projected earthquake magnitudes with the Pisarenko-Sornette 
approach of Extreme Value Theory

* Code
:PROPERTIES:
:session:  *R*
:results: silent
:exports: code 
:cache: yes 
:tangle: seismicity_gev.r
:END:

** Data acquisition 
We will need some libraries, so let us declare these first. Then, we
read in the data from file. The data provided need a little work. We are interested in
- magnitude
- UTC
- lat and long
- depth
The column names have to be tidied up a little, and the Date and Time
fields have to be converted into UTC format for R.  The ordering of
the data most convenient by ascending UTC.

#+BEGIN_SRC R
  library("data.table")
  library("dplyr")
  library("evd")
  library("geosphere")
  library("tidyr")

  events <- read.table("../data/earthquakes.csv", header = TRUE, sep = ",",
                       stringsAsFactors = FALSE)
  events <- select(events, -Sydney.Date, -Sydney.Time, -Approximate.location,
                   -EVENT.ID, -ORIGIN.ID)
  events <- unite(events, UTC, c(UTC.Date, UTC.Time), sep = " ")
  events$UTC <- as.POSIXct(events$UTC, tz = "GMT")
  events <- rename(events, Depth = Depth..km.)
  events <- arrange(events, UTC)
#+END_SRC

** Data pre-processing
Testing sensitivity to magnitude estimate errors. Just perturb the
actual magnitudes by e(0.0, 0.2)
#+BEGIN_SRC R
  My_perturb <- rnorm(length(events$Magnitude), mean=0.0, sd = 0.2)
  events$Magnitude <- events$Magnitude + My_perturb

#+END_SRC

De-clustering of the catalogue might improve the Poissonian nature of
the catalogue data, by eliminating any after-shocks.  Define two
functions to help decide whether an event falls into the aftershock
window. First we test if the event falls within the time windown, with
the Time_Check function, and then we look to see if the event falls
within the spatial reach of the main events, using the Distance_Check
function (remember that the distHaversine function returns distance in
m, not km)

The good approach here is to identify the events that are supposed to
be main shocks, and calculate both the time and space limits imposed
by the magnitude of the shock. These limits can then be used to assess
all the other events.

Proposed set of main-shock magnitudes
#+BEGIN_SRC R
  Max_Mag <- 4.5
  main_events <- filter(events, Magnitude > Max_Mag)
  main_events <- mutate(main_events,
                        delta_T = 10^(-0.31 + 0.46 * Magnitude),
                        delta_D = 10^(-0.85 + 0.46 * Magnitude))
#+END_SRC

#+BEGIN_SRC R
  Time_Check <- function(index_main,index_test)
    {
    T_1 <- events$UTC[index_main]
    T_2 <- events$UTC[index_test]
    if (T_2-T_1 < 10^(-0.31+0.46*events$Magnitude[index_main]))
        Result <- TRUE
    else Result <- FALSE
    Result
    }

  Distance_Check <- function(index_main,index_test)
    {
     P1 <- c(events$Longitude[index_main], events$Latitude[index_main])
     P2 <- c(events$Longitude[index_test], events$Latitude[index_test])
     Threshold <- 10^(-0.85+0.46*events$Magnitude[index_main])
     Distance <- distHaversine(P1, P2)/1000
     if (Distance < Threshold) Result <- TRUE
     else Result <- FALSE
     Result
    }
#+END_SRC

Add a logical field, to flag if the entry will be removed as an
aftershock.  Reset the time values to number of days since the first
event in the database.
#+BEGIN_SRC R
  events <- data.frame(events, Keep=rep(T, times = length(events$UTC)))
  events$UTC <- as.numeric(difftime(events$UTC,events$UTC[1], units="days"))
  events$UTC <- events$UTC - events$UTC[1]
#+END_SRC


Step through all the events: check if the magnitude falls in the main
shock series, then check the following events to see if these fall in
the critical time-space window

#+BEGIN_SRC R
  Index <- 1
  while (Index < length(events$UTC)-1)     # Stepping through the data matrix
    {
    Step <- 1                            # Starting the aftershock check sequence
    while
    (Index+Step < length(events$UTC) && Time_Check(Index,Index+Step) == TRUE
        && (events$Magnitude[Index] > events$Magnitude[Index+Step]))
      {
          if
          (Distance_Check(Index,Index+Step) == TRUE
              && events$Magnitude[Index] > Max_Mag)
              events$Keep[Index+Step] <- FALSE
      Step <- Step + 1
      }
    Index <- Index + 1
    }

  # The de-clustered database of events is the subset of Keep==TRUE
  events <- filter(events, Keep == TRUE)

  # Reset the time values to number of days since the first event in the database
  delta_T_Max <- events$UTC[length(events$UTC)]-events$UTC[1]
  events$UTC <- events$UTC - events$UTC[1]
#+END_SRC

Calculate the Gutenberg-Richter relationship between magnitude and
number of events smaller or equal to the given magnitude
#+BEGIN_SRC R
  GR <- count(events, Magnitude)
  GR <- mutate(GR, n = cumsum(n))
  GR <- mutate(GR, n = max(n) - n + 1)
#+END_SRC

** EVD calculations

First, set the lower magnitude cut-off
Now define time steps for the Generalised Extreme Value distribution, in days
#+BEGIN_SRC R
  M_Min <- 3.5
  delta_T <- 10
  Time_Steps <- seq(20, 300, by=delta_T)
#+END_SRC

Set up the number of data shuffles to bootstrap the GEV parameters and
improve their accuracy (i.e., reduce variability)
#+BEGIN_SRC R
  Bootstrap_Total <- 100
  shuffle_events <- events
#+END_SRC
 
The fitted parameters go into an 3-d array
#+BEGIN_SRC R
  GEV_Parameters <- array(0, c(length(Time_Steps),4,Bootstrap_Total))

  for (Re_runs in 1:Bootstrap_Total)
    {# The idea behind the bootstap approach is that shuffling the magnitudes
    # around amounts to a resampling of the population of the events, whilst
    # maintaining the distribution in time
    shuffle_events$Magnitude <- sample(events$Magnitude)
    # We need to step through the entire events dataset in contiguous blocks
    # of size Time_Steps[i], and determine the maximum magnitude in each of
    # the intervals.

    for (i in 1:length(Time_Steps))
      {# Looping over the Time Intervals
      # Determine the contents of the successive time bins through the hist function:
      # use the number in each bin (and accumulate) to find the position of the data
      # entries in the events matrix
      my_breaks <- seq(0,delta_T_Max+Time_Steps[i],by=Time_Steps[i])
      Time_Hist <- hist(shuffle_events$UTC,breaks=my_breaks,plot=F)
    
      # The numbers in each bin are stored in Time_Hist$counts and can be used now
      # to calculate the maximum magnitude encountered in each of the time bins
      Bin_low <- 0
      Bin_high <- 0
      Bin_Max_Magnitudes <- rep(0, times=length(Time_Hist$counts))
    
      for (Bins in 1:length(Time_Hist$counts))
        {
        Bin_high <- Bin_low + Time_Hist$counts[Bins]
        Bin_Max_Magnitudes[Bins] <- max(shuffle_events$Magnitude[Bin_low:Bin_high])
        if (Bin_Max_Magnitudes[Bins] < M_Min) Bin_Max_Magnitudes[Bins] <- NA
        Bin_low <- Bin_high
        }

      # Calculate the MLE of the GEV distribution and store the results
      # The order is: T, loc, scale, shape, error_loc, error_scale, error_shape
      GEV_Fit <- fgev(Bin_Max_Magnitudes,std.err=F)
      GEV_Parameters[i, ,Re_runs] <- c(Time_Steps[i],fitted.values(GEV_Fit))
      }# End of Time Interval Looping  
    }
#+END_SRC

Present the results by performing a statistical summary of the
parameter estimates
#+BEGIN_SRC R
  GEV_Results <- array(0, c(length(Time_Steps),10))

  for (i in 1:length(Time_Steps))
    { 
    GEV_Results[i,1] <- Time_Steps[i]
    GEV_Results[i,2:4] <- quantile(GEV_Parameters[i,2,],probs=c(0.16,0.50,0.84))
    GEV_Results[i,5:7] <- quantile(GEV_Parameters[i,3,],probs=c(0.16,0.50,0.84))
    GEV_Results[i,8:10] <- quantile(GEV_Parameters[i,4,],probs=c(0.16,0.50,0.84))
    }

  Shape <- GEV_Results[,9]  
  Scale <- GEV_Results[,6]
  Location <- GEV_Results[,3]
#+END_SRC

Now we can calculate estimates of maximum magnitudes for arbitrary
time in the future

#+BEGIN_SRC R
  Tau <- c(365000*1:5)
  Q <- 0.975

  Quantiles <- array(0, c(length(Time_Steps),length(Tau)))
  for (Tau_i in 1:length(Tau))
    {
    Quantiles[,Tau_i] <- Location + ((Tau[Tau_i]/(log(1/Q)*Time_Steps))^Shape - 1) * Scale / Shape
    }
#+END_SRC

** Plot results

Let's test how well the reduced data set fulfills the Poisson Process
#+BEGIN_SRC R
  Test_Data <- subset(events, events$Magnitude > M_Min, select=UTC)
  Test_Data$UTC  <- Test_Data$UTC - Test_Data$UTC[1]
  NOE <- length(Test_Data$UTC)
  TL <- Test_Data$UTC[NOE]
  # Generate the equivalent Poissonian data set
  Poisson_Data <- seq(1, TL, by=TL/NOE)
  #  and carry out the Kolmogorov-Smirnov test
  Test_Result <- ks.test(events$UTC,Poisson_Data)
#+END_SRC

Make the actual plots
#+BEGIN_SRC R
  pdf(file="earthquakes_gev.pdf",paper="a4",width=0,height=0,pointsize=10)
  op <- par(mfrow = c(3,2))
  plot(GR,log="y",main="Gutenberg-Richter Plot");grid(lty=2,col=5)
  abline(v=M_Min,col=2)
  My_List <- subset(events, Magnitude > M_Min, select = c(UTC,Magnitude))
  plot(My_List$UTC/My_List$UTC[length(My_List$UTC)],type="l",xlab="Event Number",ylab="Normalised Occurrence Time",main="Poisson Fit");grid(lty=2,col=5)
  abline(0,1/length(My_List$UTC),col=4)
  text(NOE/5,0.8,"p-value:")
  text(NOE/5,0.7,round(Test_Result$p.value,5))
  matplot(GEV_Results[,1],GEV_Results[,8:10],type="l",lty=1,col=c(1,2,1),main="GEV Parameter Estimation",xlab="T Window (days)", ylab="Shape Parameter");grid(lty=2,col=5)
  matplot(GEV_Results[,1],GEV_Results[,5:7],type="l",lty=1,col=c(1,2,1),main="GEV Parameter Estimation",xlab="T Window (days)", ylab="Scale Parameter");grid(lty=2,col=5)
  matplot(GEV_Results[,1],GEV_Results[,2:4],type="l",lty=1,col=c(1,2,1),main="GEV Parameter Estimation",xlab="T Window (days)", ylab="Location Parameter");grid(lty=2,col=5)
  matplot(Time_Steps,Quantiles,ylim=c(6,9),type="l",lty=1,col=1,main="GEV Maximum Magnitude Estimation",xlab="T Window (days)", ylab="0.975 Magnitude Quantile");grid(lty=2,col=5)
  par(op)
  dev.off()
#+END_SRC
