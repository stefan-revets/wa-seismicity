#+TITLE: Seismicity Statistics \\ using Extreme Value Theory
#+OPTIONS: toc:nil ^:{}
#+LATEX_HEADER: \usepackage{chicago,SIunits}

* Introduction
Probabilistic Seismic Hazard Analysis (PSHA) relies fundamentally on
earthquake statistics and on rock and soil mechanics. Earthquake
statistics provide the magnitude-recurrence relations. Rock and soil
mechanics provide the ground motion response to a given earthquake.
Measurements, correlations and modelling rely almost exclusively on
events in tectonically active regions, most notably California
\cite{gutenberg-richter44:frequency,knopoff_al82:b-values,kaklamanos_al10:implementation}.
In contrast, the earthquakes occurring on the Australian continent are
a prime example of intra-plate seismicity. The processes and drivers
of intra-plate seismicity are and remain enigmatic. However, it is
clear that these processes are very different from those driving the
tectonic earthquakes \cite{stein07:approaches}. Ground motion
predictions are empirically derived equations (for an overview, see
\citeNP{kaklamanos_al10:implementation}), based on data collected over
rock and over siliciclastic sediments, largely from observations in
the continental Americas. There have been some attempts to redress
this situation for Australian soils, but the models are commonly
similar to the American ones
(\cite{lam_wilson08:new,leonard-al07:model}). No implementations have
been developed specifically for calcareous sediments.  A number of
separate but linked investigations are reported upon here with the aim
to help and address some of the shortcomings currently plaguing PSHA
on the Northwest Shelf.

* Data Sources
The earthquake data used in this study are taken from the publicly
accessible Geoscience Australia earthquake catalogue (available
on-line from www.ga.gov.au). The catalogue lists the timing, location,
and magnitude of earthquakes in Australia going back to 1888. The
vexed issue of completeness has been discussed by
\citeNP{leonard08:hundred} and also
\citeNP{sagar_leonard07:mapping}. These authors also discuss issues
encountered with the value of magnitude as recorded over the years,
and in particular changes to the scales and the way in which these
changes have been applied.  A subset of this catalogue was defined to
include events which have occurred in an area encompassing the
Northwest Shelf (16\degree--23\degree S, 112\degree--124\degree E;
see Figure 1). The magnitudes have been corrected as discussed below.

* Statistics of Magnitude
One of the important regularities found to govern the complex
process of seimic activity is a relationship between size and
frequency of earthquakes. The Gutenberg-Richter equation states
that 
\begin{equation}
  \label{eq:gutenberg-richter}
N_T(M) = 10^{a - b M} 
\end{equation}
where $N$ is the number of events with magnitude larger than $M$ over
a sufficiently long period of time $T$
\cite{gutenberg-richter44:frequency}. While this equation has been
widely and successfully applied, it suffers from limitations.
Gutenberg-Richter plots always show a more or less developed
``shoulder'' towards the lower end of the magnitude axis, partly due
to under reporting of lower magnitude earthquakes. Towards the higher
end of the magnitude axis, the scatter of the data points increases
considerably, due to the paucity of the largest earthquakes. In many
instances, a systematic convexity of the magnitude-frequency line can
be seen. These general observations provide a warning that the
statistics of a regression analysis will be suspect.  A further
significant limitation is that the relationship shows no (upper)
limit: it allows extrapolations to be made to earthquakes with an
(arbitrarily) large magnitude. The finite amount of energy stored in
the Earth, and the finite strength of rocks dictate that there must be
an upper limit to the magnitude of an earthquake. More or less
significant deviations from the Gutenberg-Richter formula have been
documented for the largest magnitudes (e.g.,
\citeNP{pisarenko_sornette04:rigorous}). Two modifications (``corner''
magnitude and maximum magnitude) have been proposed and are often
adopted. Their shortcomings have led to the adaptation and adoption of
the more appropriate, statistically sound methods from the study of
extremal events (\cite{embrechts_al97:extremal}).

** ``Corner'' magnitude

\citeNP{kagan97:seismic,kagan_schoenberg01:estimation} and
\citeNP{vere-jones_etal01:remarks} multiplied the power law distribution
of the seismic moments (which corresponds to the Gutenberg-Richter
distribution of magnitudes) by an exponential taper. This results in
either a $\Gamma$ or a modified Pareto distribution, with a characteristic
moment, or ``corner'' magnitude equivalent in the corresponding
magnitude distribution. The effect is a ``soft'' truncation of the
Gutenberg-Richter law.

** Maximum magnitude

The proposal to set a maximum possible earthquake size $M_\mathrm{max}$, a
hard truncation of the Gutenberg-Richter law
\cite{consentino_al77:truncated,dargahi-noubary83:procedure,main_al99:constraints}
plays an important role in seismic risk and seismic hazard studies
\cite{bender_perkins93:_treatment,cornell94:statistical,kijiko_graham98:parametric}. It
provides a very attractive measure for engineers and insurers alike,
as risk and construction standards can be set against a given maximum
magnitude.  Unfortunately, the estimation or calculation of $M_{max}$
remains unsatisfactory. Its superficial attraction is undermined by
undesirable features
\cite{kagan93:statistics,pisarenko_etal08:new_approach}:
\begin{enumerate}
\item $M_\mathrm{max}$ is ill-defined, as it does not contain the time scale over
  which it has been determined, or over which is valid.
\item The cut-off nature of $M_\mathrm{max}$ is arbitrary in the sense that
  the impossibility of $M_\mathrm{max} + \epsilon$ for any arbitrarily small
  value of $\epsilon$ has no (physical) justification.
\item $M_\mathrm{max}$ is statistically highly unstable.
\end{enumerate}

** Extreme Value Theory

Extreme value theory provides the theory to handle extremes of random
phenomena. In contrast to basic statistics, it provides the necessary
tools to deal with statistical distribution issues such as skewness,
fat tails, rare events and the like \cite{embrechts_al97:extremal}.
\citeNP{pisarenko_etal08:characterisation} applied with success the
Generalised Extreme Value distribution (GEV) and the Generalised
Pareto distribution (GPD) in their attempts to characterise the
distribution of earthquake magnitudes and go beyond the limitations of
the Gutenberg-Richter relation and its various ad-hoc modifications.
The Frechet-Fisher-Tipper theorem \cite{embrechts_al97:extremal} leads
to the definition of the GEV distribution
\begin{equation}
  \label{eq:GEV}
\Phi(x|\mu,\sigma,\xi) = e^{- (1 + \xi (x - \mu)/ \sigma)^{-1/\xi)}},  
\end{equation}
a Pareto ($ξ>0$) or a Weibull ($ξ<0$) distribution which
degenerates for $\xi = 0$ to 
\begin{equation}
  \label{eq:gumbel}
\Phi(x|\mu,\sigma) = e^{-e^{-(x - \mu)/ \sigma}},  
\end{equation}
(Gumbel distribution) in which $\mu$, $\sigma$, $\xi$ are respectively
the centering, scale and shape parameters. The theorem provides the
statistical and mathematical justification for this definition of the
limiting distribution of the maxima of identically independently
distributed random variables x as the sample size n goes to infinity.
The quantiles $Q_q$ of the GEV distribution $\Phi(x|\mu,\sigma,\xi)$ are
\begin{equation}
  \label{eq:quantiles}
Q_q(\tau) = \mu(T) + ((\tau/T \log(1/q))^{\xi} - 1) \sigma(T)/\xi  
\end{equation}
with $q$ the quantile confidence level, $\tau$ an arbitrary time
interval (in the future) and $T$ the time interval step used to
estimate the parameters of the GEV distribution.  The utility and
value of the quantiles over $M_\mathrm{max}$ as stable and statistically
meaningful values was demonstrated and illustrated by
\citeNP{pisarenko_etal08:characterisation,pisarenko_etal08:new_approach}.
In practice, the parameters of the GEV distribution can be estimated
from data through maximum likelihood calculations. 

* Statistics of Recurrence Rates

The analysis and calculation of recurrence rates or waiting times of
seismic events is beset with difficulties equivalent to those
encountered in the study of magnitude distribution. It is clear that
the obvious extension of the Gutenberg-Richter magnitude-number
relation to a magnitude-frequency relation by simply dividing the
numbers by the time interval, compounds the statistical problems just
discussed by bringing in explicitly the time dimension. Size, number
and timing of seismic events are related, but the nature of the
relationships is complex and not fully understood.  A radically
different approach was proposed by \citeNP{bak_al02:unified}, and
further developed and refined by
\citeNP{corral05:renormalisation,corral06:dependence,corral09:statistical}. Their
application of the theories of self-organised criticality led to a
modified $\Gamma$ distribution, so that
\begin{equation}
  \label{eq:mod_gamma}
  p(x) = C \delta x^{\gamma-1} e^{-(x/\alpha)^{\delta}} / \alpha^\gamma \Gamma(\gamma/\delta)   
\end{equation}
where $x = \lambda t$ (the average seismic rate multiplied by time).

\citeNP{saichev_sornette06:universal} criticised this analysis as
incompatible and at variance with the data and they promoted the
Epidemic-Type Aftershock Sequence as a better approach.  The
Epidemic-Type Aftershock Sequence (ETAS) was proposed by
\citeNP{kagan_knopoff81:stochastic} and \citeNP{ogata88:statistical}, with
the statistical and mathematical properties studied and expanded
further over the years (see \citeNP{saichev_sornette07:theory} for an
overview). It integrates the Gutenberg-Richter relationship with the
Omori relationship of aftershock sequences, and adds a productivity
law and a measure of the fractal nature of fault networks.
\begin{equation}
  \label{eq:etas}
p(x) = (\alpha n \theta \rho^\theta x^{-1-\theta} + (1 - n + \alpha n \rho^\theta x^{-\theta})^2) e^{( -(1 - n) x - \alpha n \theta \rho^\theta x^{1-\theta}/(1-\theta))}
\end{equation}
where $x = \lambda t$ (the average seismic rate multiplied by time),
$\alpha = (\lambda_0 c)^\theta$ (reflecting the Omori aftershock law),
$\rho = \lambda/ \lambda_0 = Q(m) (L/L_0)^d$ (a measure of the
productivity law), and with $n$ the criticality parameter. The function
accounts much better for the observations, as can be expected from a
4-parameter function.  The parameters of the function can be fitted to
the data through maximum likelihood calculations. 

* Calculations
:PROPERTIES:
:session:  *R*
:results: output graphics
:exports: both
:cache: yes 
:tangle: seismicity_gev.r
:END:      

** Data acquisition 
We will need some libraries, so let us declare these first. Then, we
read in the data from file. The data provided need a little work. We
are interested in
- magnitude
- UTC
- lat and long
- depth
The column names have to be tidied up, and the Date and Time fields
have to be converted into UTC format for R.  The ordering of the data
by ascending UTC will be useful later on. Add a logical field, to flag
if an events might be an aftershock.

#+BEGIN_SRC R
  library("data.table")
  library("dplyr")
  library("evd")
  library("geosphere")
  library("tidyr")

  events <- read.table("../data/earthquakes.csv", header = TRUE,
                       sep = ",", stringsAsFactors = FALSE)
  events <- select(events, -Sydney.Date, -Sydney.Time,
                   -Approximate.location, -ORIGIN.ID)
  events <- unite(events, UTC, c(UTC.Date, UTC.Time), sep = " ")
  events$UTC <- as.POSIXct(events$UTC, tz = "GMT")
  events <- arrange(events, UTC)

  events <- rename(events,
                   Mag = Magnitude,
                   Long = Longitude,
                   Lat = Latitude,
                   Depth = Depth..km.)
  events <- mutate(events, Aftershock = FALSE)

#+END_SRC

** Magnitudes and Timing
Before we calculate the various properties we are interested in, it is
a good idea to run a few checks and put in place a number of
opportunities to test any boundary conditions which may or may not be
fulfilled.

First, let us make a simple plot of the data
#+BEGIN_SRC R :file ../output/time_mag.pdf
  plot(events$UTC,jitter(events$Mag),
       main = "Earthquakes recorded over time",
       xlab = "Date",
       ylab = "Magnitude",
       pch = 20,
       col = "blue")
  grid()

#+END_SRC

#+RESULTS[c9b582df1c2432202c50e929edd6fba7e1e282d4]:
[[file:../output/time_mag.pdf]]

The plot shows that the database is not uniform: far fewer events have
been recorded in earlier years.

*** Catalogue Magnitude
\citeNP{sagar_leonard07:mapping} discussed the magnitude determination
and the way this has changed over the years. They illustrated these
changes with a figure of the pre- and post-1992 Western Australian
earthquake magnitude Cumulative Density graph, including an equivalent
graph with the South Australian earthquakes.  Let us check if this
discrepancy is still the case with the current data file from NW
Australia.  

We can split the events into pre- and post-1992 groups, plot their
Gutenberg-Richter relation separately. As we can see, there is indeed
a systematic difference of about 0.5 magnitudes. 

#+BEGIN_SRC R :file ../output/mag_issues.pdf
  GR_pre <- events %>%
      filter(year(UTC) < 1992) %>%
      count(Mag) %>%
      mutate(n = cumsum(n)) %>%
      mutate(n = max(n) - n + 1)

  GR_post <- events %>%
      filter(year(UTC) > 1991) %>%
      count(Mag) %>%
      mutate(n = cumsum(n)) %>%
      mutate(n = max(n) - n + 1)

  GR_pre_corr<- events %>%
      filter(year(UTC) < 1992) %>%
      mutate(Mag = Mag - 0.5) %>%
      count(Mag) %>%
      mutate(n = cumsum(n)) %>%
      mutate(n = max(n) - n + 1)

  plot(GR_pre, log = "y", pch = 19,
       main = "Gutenberg-Richter relation")
  points(GR_post)
  points(GR_pre_corr, col = "green")
  grid()
#+END_SRC

#+RESULTS[5fe29b5c7157ae7b39766f3d5e882603eed4c25b]:
[[file:../output/mag_issues.pdf]]

We therefore change the magnitude values of all pre-1992 events
accordingly.

#+BEGIN_SRC R
  events$Mag[(year(events$UTC) < 1992)] <-
      events$Mag[(year(events$UTC) < 1992)]-0.5
#+END_SRC

A more informative, quantitative way of showing any such changes is
through a cumulative density plot
#+BEGIN_SRC R :file ../output/ecdf.pdf
  events <- mutate(events,
                   UTC_d = as.numeric(difftime(UTC,UTC[1],
                                               unit = "days")))
  plot(ecdf(events$UTC_d/365.4 + year(events$UTC[1])),
       main = "Cumulative distribution of earthquakes",
       xlab = "Year",
       pch = 20,
       col = "blue")
  grid()

#+END_SRC

#+RESULTS[4b89c80ed3c23bf5019cb02bfb538efcb67a4dae]:
[[file:../output/ecdf.pdf]]

This graph shows that the events before 1979 are much sparser, and
hence will interfere significantly with any statistical
calculations. A careful look also reveals that another change takes
place around 2002, when once again fewer events make up the database.

*** Stationary, Poissonian, process

The statistical techniques applied in this study assume a
stationary Poisson process. A plot of all events in the catalogue
against time shows immediately that the process is not
stationary (see Figure 3). A stationary period appears to be
present between May 1979 and November 2002 (for a total of 485
events): a plot of events against time yields the expected, and
required, linear relation (see Figure 4).

Let us therefore restrict our data and exclude all events before 1979
and after 2002. While we do this, let us take the opportunity of
adding a column with a time line, as the number of days elapsed since
the first event in the (filtered) database. Show also the result of
this restriction with a cumulative distribution plot.

#+BEGIN_SRC R :file ../output/ecdf_restricted.pdf
  events <- events %>%
      filter(year(UTC) > 1979, year(UTC) < 2003) %>%
      mutate(UTC_d = as.numeric(difftime(UTC,UTC[1], unit = "days")))

  plot(ecdf(events$UTC_d/365.4 + year(events$UTC[1])),
       main = "Cumulative distribution of earthquakes",
       xlab = "Year",
       pch = 20,
       col = "blue")
  grid()

#+END_SRC

#+RESULTS[78afea68aad67fcb8a8ed7192b5ab4e070dc3a51]:
[[file:../output/ecdf_restricted.pdf]]

We can verify to what extent this restricted data set is consistent
with a Poisson Process and use the Kolmogorov-Smirnov test.
#+BEGIN_SRC R
  NOE <- length(events$UTC_d)
  TL <- max(events$UTC_d)
  Poisson_Data <- seq(1, TL, by=TL/NOE)
  KS_Result <- ks.test(events$UTC_d,Poisson_Data)
  KS_Result

#+END_SRC

#+RESULTS[31891a07da47d8ed6d413d80bda84f54dce00385]:
: Warning message:
: In ks.test(events$UTC_d, Poisson_Data) :
:   p-value will be approximate in the presence of ties
: 
: 	Two-sample Kolmogorov-Smirnov test
: 
: data:  events$UTC_d and Poisson_Data
: D = 0.057026, p-value = 0.4017
: alternative hypothesis: two-sided

** Sensitivity to data error
We may gain some idea of the sensitivity of the results to magnitude
estimate errors. The simplest way of doing this, is by perturbing the
actual magnitudes by e(0.0, 0.25) and re-running the entire analysis.
#+BEGIN_SRC R :tangle no
#  My_perturb <- rnorm(length(events$Mag), mean=0.0, sd = 0.25)
#  events$Mag <- events$Mag + My_perturb

#+END_SRC

** De-clustering
De-clustering of the catalogue might improve the Poissonian nature of
the catalogue data, by eliminating any after-shocks.  

We will do this by identifying the events that are deemed to be main
shocks, and calculating both the time and space intervals imposed by
the magnitude of each of the shocks.

We then check each event in the catalogue against all these limits: if
an event falls within the limits, it gets flagged as an aftershock.

First, we set the main-shock magnitudes, with the calculated time and
distance limits imposed by the actual magnitude (the values of
parameters used to determine the size of the windows are taken from
\cite{knopoff_al82:b-values})
#+BEGIN_SRC R
  Max_Mag <- 5.0
  main_shocks <- filter(events, Mag > Max_Mag)
  main_events <- mutate(main_shocks,
                        delta_T = 10^(-0.31 + 0.46 * Mag),
                        delta_D = 10^(-0.85 + 0.46 * Mag))

#+END_SRC

#+RESULTS[7dd7ad7aa4b4a08f684df37c229a77696a4cf4dc]:

Step through all the events: check if the magnitude falls in the main
shock series, then check the following events to see if these fall in
the critical time-space window (remember that the distHaversine
function returns distance between points (long, lat) in m, not km).
To avoid clashes with main shocks, we shall take out the main shocks
from the events set, and then add them again after processing.

#+BEGIN_SRC R
  events <- anti_join(events, main_shocks, by = "EVENT.ID")

  Index <- 1
    while (Index < length(events$UTC)-1)
    {
        main_result <- main_events %>%
            mutate(test_dT = as.numeric(difftime(events$UTC[Index], UTC,
                                                 unit = "days")),
                   test_dD = distHaversine(cbind(Long, Lat),
                                           c(events$Long[Index], events$Lat[Index]))/1000
                   ) %>%
            filter(test_dT > 0, test_dT < delta_T, test_dD < delta_D)
        if (nrow(main_result) > 0) {events$Aftershock[Index] = TRUE}
        
      Index <- Index + 1
      }

  events <- full_join(events, main_shocks)
  events <- arrange(events, UTC)

  events <- filter(events, Aftershock == FALSE)

#+END_SRC

#+RESULTS[966e3f08614a7f70a13e6fbb84e07edd6d209a47]:

** EVD calculations

First, set the lower magnitude cut-off. We also have to define the
time steps for the Generalised Extreme Value distribution (in days)
#+BEGIN_SRC R
  M_Min <- 2.5
  delta_T <- 10
  Time_Steps <- seq(20, 300, by = delta_T)
  delta_T_Max <- last(events$UTC_d) - first(events$UTC_d)

#+END_SRC

#+RESULTS[dc5844e1c70cff3605a61cf572e2dd012f543978]:

To improve the acuracy of the results, a bootstrapping approach is
highly effective. The idea behind the bootstap approach is that
shuffling the magnitudes around amounts to a resampling of the
population of the events, whilst maintaining the distribution in time.
Set up the number of data shuffles to bootstrap the GEV parameter
calculations.

The fitted parameters go into an 3-d array

We need to step through the entire events dataset in contiguous blocks
of size Time_Steps[i], and determine the maximum magnitude in each of
the intervals. A convenient way of doing this is by creating
additional columns, containing the blocknumber (in effect the modulus
of the day number of the event to the time step size). These numbers
can then be used as groups, so that dplyr grouping can be brought into
play. 

Calculate the MLE of the GEV distribution and store the results The
order is: T, loc, scale, shape, error_loc, error_scale, error_shape

#+BEGIN_SRC R
  Bootstrap_Total <- 100
  shuffle_events <- events

  GEV_Parameters <- array(0, c(length(Time_Steps),4,Bootstrap_Total))

  for (Re_runs in 1:Bootstrap_Total){
          shuffle_events$Mag <- sample(events$Mag)
          for (i in 1:length(Time_Steps)){
              shuffle_events <- shuffle_events %>%
                  mutate(block = UTC_d %/% Time_Steps[i])
              Max_Mags <- shuffle_events %>%
                  group_by(block) %>%
                  summarize(value = max(Mag))
              GEV_Fit <- fgev(Max_Mags$value,std.err=F)
              GEV_Parameters[i, ,Re_runs] <-
                  c(Time_Steps[i],fitted.values(GEV_Fit))
          }
  }

#+END_SRC

#+RESULTS[19c7aaa71371b8140a3e1a6a92181e930a1527ac]:

Present the results by performing a statistical summary of the
parameter estimates
#+BEGIN_SRC R
  GEV_Results <- array(0, c(length(Time_Steps),10))

  for (i in 1:length(Time_Steps))
    { 
    GEV_Results[i,1] <- Time_Steps[i]
    GEV_Results[i,2:4] <- quantile(GEV_Parameters[i,2,],
                                   probs=c(0.16,0.50,0.84))
    GEV_Results[i,5:7] <- quantile(GEV_Parameters[i,3,],
                                   probs=c(0.16,0.50,0.84))
    GEV_Results[i,8:10] <- quantile(GEV_Parameters[i,4,],
                                    probs=c(0.16,0.50,0.84))
    }

  Shape <- GEV_Results[,9]  
  Scale <- GEV_Results[,6]
  Location <- GEV_Results[,3]
#+END_SRC

#+RESULTS[506c1cc872509c3e488e33f253e6f325f37a4697]:

Now we can calculate estimates of maximum magnitudes for arbitrary
time in the future. Here we look at 1000, 2000, 3000, 4000 and 5000
years ahead

#+BEGIN_SRC R
  Tau <- c(365000*1:5)
  Q <- 0.975

  Quantiles <- array(0, c(length(Time_Steps),length(Tau)))
  for (Tau_i in 1:length(Tau))
    {
        Quantiles[,Tau_i] <- Location +
            ((Tau[Tau_i]/(log(1/Q)*Time_Steps))^Shape - 1) * Scale / Shape
    }
#+END_SRC

#+RESULTS[1035e11f5b32ba5657ef0fc3a00ad62fdad1f767]:

** Plot results

Calculate the Gutenberg-Richter relationship between magnitude and
number of events smaller or equal to the given magnitude
#+BEGIN_SRC R
  GR <- events %>%
      count(Mag) %>%
      mutate(n = cumsum(n)) %>%
      mutate(n = max(n) - n + 1)

  #GR <- count(events, Mag)
  #GR <- mutate(GR, n = cumsum(n))
  #GR <- mutate(GR, n = max(n) - n + 1)
#+END_SRC

#+RESULTS[b72396fbfa13050372cb7224aea688b9cc99975a]:

Make the actual plots
#+BEGIN_SRC R :file ../output/earthquakes_gev.pdf
  op <- par(mfrow = c(3,2))

  plot(GR,log="y",main="Gutenberg-Richter Plot")
  grid(lty=2,col=5)
  abline(v=M_Min,col=2)
  My_List <- subset(events, Mag > M_Min, select = c(UTC_d,Mag))

  plot(My_List$UTC_d/My_List$UTC_d[length(My_List$UTC_d)],
       type="l",
       xlab="Event Number",
       ylab="Normalised Occurrence Time",
       main="Poisson Fit")
  grid(lty=2,col=5)
  abline(0,1/length(My_List$UTC_d),col=4)
  text(NOE/5,0.8,"p-value:")
  text(NOE/5,0.7,round(KS_Result$p.value,5))

  matplot(GEV_Results[,1],GEV_Results[,8:10],
          type="l",
          lty=1,
          col=c(1,2,1),
          main="GEV Parameter Estimation",
          xlab="T Window (days)",
          ylab="Shape Parameter")
  grid(lty=2,col=5)

  matplot(GEV_Results[,1],GEV_Results[,5:7],
          type="l",
          lty=1,
          col=c(1,2,1),
          main="GEV Parameter Estimation",
          xlab="T Window (days)",
          ylab="Scale Parameter")
  grid(lty=2,col=5)

  matplot(GEV_Results[,1],GEV_Results[,2:4],
          type="l",
          lty=1,
          col=c(1,2,1),
          main="GEV Parameter Estimation",
          xlab="T Window (days)",
          ylab="Location Parameter")
  grid(lty=2,col=5)

  matplot(Time_Steps,Quantiles,ylim=c(6,9),
          type="l",
          lty=1,
          col=1,
          main="GEV Maximum Magnitude Estimation",
          xlab="T Window (days)",
          ylab="0.975 Magnitude Quantile")
  grid(lty=2,col=5)

  par(op)
#+END_SRC

#+RESULTS[88eb074649394b7da4f28848661c880faaa29200]:
[[file:../output/earthquakes_gev.pdf]]

\bibliography{biblio_seismology}
\bibliographystyle{chicago}
