#+TITLE: Seismicity Statistics through Extreme Value Theory

* Introduction
Calculate the projected earthquake magnitudes with the Pisarenko-Sornette 
approach of Extreme Value Theory

* Code
:PROPERTIES:
:session:  *R*
:results: silent
:exports: code 
:cache: yes 
:tangle: seismicity_gev.r
:END:      

** Data acquisition 
We will need some libraries, so let us declare these first. Then, we
read in the data from file. The data provided need a little work. We
are interested in
- magnitude
- UTC
- lat and long
- depth
The column names have to be tidied up, and the Date and Time fields
have to be converted into UTC format for R.  The ordering of the data
by ascending UTC will be useful later on. Add a logical field, to flag
if an events might be an aftershock.

#+BEGIN_SRC R
  library("data.table")
  library("dplyr")
  library("evd")
  library("geosphere")
  library("tidyr")

  events <- read.table("../data/earthquakes.csv", header = TRUE, sep = ",",
                       stringsAsFactors = FALSE)
  events <- select(events, -Sydney.Date, -Sydney.Time, -Approximate.location,
                   -ORIGIN.ID)
  events <- unite(events, UTC, c(UTC.Date, UTC.Time), sep = " ")
  events$UTC <- as.POSIXct(events$UTC, tz = "GMT")
  events <- arrange(events, UTC)

  events <- rename(events,
                   Mag = Magnitude,
                   Long = Longitude,
                   Lat = Latitude,
                   Depth = Depth..km.)
  events <- mutate(events, Aftershock = FALSE)

#+END_SRC

** Data pre-processing
Before we calculate the various properties we are interested in, it is
a good idea to run a few checks and put in place a number of
opportunities to test any boundary conditions which may or may not be
fulfilled.

Testing sensitivity to magnitude estimate errors. Just perturb the
actual magnitudes by e(0.0, 0.2)
#+BEGIN_SRC R
  My_perturb <- rnorm(length(events$Mag), mean=0.0, sd = 0.2)
  events$Mag <- events$Mag + My_perturb

#+END_SRC

De-clustering of the catalogue might improve the Poissonian nature of
the catalogue data, by eliminating any after-shocks.  

We will do this by identifying the events that are deemed to be main
shocks, and calculating both the time and space limits imposed by the
magnitude of each of the shocks. 

We then check each event in the catalogue against all these limits: if
it falls within the limits, it gets flagged as an aftershock.

Proposed set of main-shock magnitudes, with the calculated time and
distance limits imposed by the actual magnitude (values of parameters
determing the size of the windows are taken from
\cite{knopoff_al82:b-values})
#+BEGIN_SRC R
  Max_Mag <- 5.0
  main_shocks <- filter(events, Mag > Max_Mag)
  main_events <- mutate(main_shocks,
                        delta_T = 10^(-0.31 + 0.46 * Mag),
                        delta_D = 10^(-0.85 + 0.46 * Mag))
#+END_SRC

Step through all the events: check if the magnitude falls in the main
shock series, then check the following events to see if these fall in
the critical time-space window (remember that the distHaversine
function returns distance between points (long, lat) in m, not km).
We shall also take out the main shocks from the events set, and then
add them again after processing.

#+BEGIN_SRC R
  events <- anti_join(events, main_shocks, by = "EVENT.ID")

  Index <- 1
    while (Index < length(events$UTC)-1)
    {
        main_result <- main_events %>%
            mutate(test_dT = as.numeric(difftime(events$UTC[Index], UTC,
                                                 unit = "days")),
                   test_dD = distHaversine(cbind(Long, Lat),
                                           c(events$Long[Index], events$Lat[Index]))/1000
                   ) %>%
            filter(test_dT > 0, test_dT < delta_T, test_dD < delta_D)
        if (nrow(main_result) > 0) {events$Aftershock[Index] = TRUE}
        
      Index <- Index + 1
      }

  events <- full_join(events, main_shocks)
  events <- arrange(events, UTC)

  #  events <- filter(events, Aftershock == FALSE)


#+END_SRC

Calculate the Gutenberg-Richter relationship between magnitude and
number of events smaller or equal to the given magnitude
#+BEGIN_SRC R
  GR <- count(events, Mag)
  GR <- mutate(GR, n = cumsum(n))
  GR <- mutate(GR, n = max(n) - n + 1)
#+END_SRC

** EVD calculations

First, set the lower magnitude cut-off
Now define time steps for the Generalised Extreme Value distribution, in days
#+BEGIN_SRC R
  M_Min <- 3.5
  delta_T <- 10
  Time_Steps <- seq(20, 300, by=delta_T)
#+END_SRC

Set up the number of data shuffles to bootstrap the GEV parameters and
improve their accuracy (i.e., reduce variability)
#+BEGIN_SRC R
  Bootstrap_Total <- 100
  shuffle_events <- events
#+END_SRC
 
The fitted parameters go into an 3-d array
#+BEGIN_SRC R
  GEV_Parameters <- array(0, c(length(Time_Steps),4,Bootstrap_Total))

  for (Re_runs in 1:Bootstrap_Total)
    {# The idea behind the bootstap approach is that shuffling the magnitudes
    # around amounts to a resampling of the population of the events, whilst
    # maintaining the distribution in time
    shuffle_events$Magnitude <- sample(events$Magnitude)
    # We need to step through the entire events dataset in contiguous blocks
    # of size Time_Steps[i], and determine the maximum magnitude in each of
    # the intervals.

    for (i in 1:length(Time_Steps))
      {# Looping over the Time Intervals
      # Determine the contents of the successive time bins through the hist function:
      # use the number in each bin (and accumulate) to find the position of the data
      # entries in the events matrix
      my_breaks <- seq(0,delta_T_Max+Time_Steps[i],by=Time_Steps[i])
      Time_Hist <- hist(shuffle_events$UTC,breaks=my_breaks,plot=F)
    
      # The numbers in each bin are stored in Time_Hist$counts and can be used now
      # to calculate the maximum magnitude encountered in each of the time bins
      Bin_low <- 0
      Bin_high <- 0
      Bin_Max_Magnitudes <- rep(0, times=length(Time_Hist$counts))
    
      for (Bins in 1:length(Time_Hist$counts))
        {
        Bin_high <- Bin_low + Time_Hist$counts[Bins]
        Bin_Max_Magnitudes[Bins] <- max(shuffle_events$Magnitude[Bin_low:Bin_high])
        if (Bin_Max_Magnitudes[Bins] < M_Min) Bin_Max_Magnitudes[Bins] <- NA
        Bin_low <- Bin_high
        }

      # Calculate the MLE of the GEV distribution and store the results
      # The order is: T, loc, scale, shape, error_loc, error_scale, error_shape
      GEV_Fit <- fgev(Bin_Max_Magnitudes,std.err=F)
      GEV_Parameters[i, ,Re_runs] <- c(Time_Steps[i],fitted.values(GEV_Fit))
      }# End of Time Interval Looping  
    }
#+END_SRC

Present the results by performing a statistical summary of the
parameter estimates
#+BEGIN_SRC R
  GEV_Results <- array(0, c(length(Time_Steps),10))

  for (i in 1:length(Time_Steps))
    { 
    GEV_Results[i,1] <- Time_Steps[i]
    GEV_Results[i,2:4] <- quantile(GEV_Parameters[i,2,],probs=c(0.16,0.50,0.84))
    GEV_Results[i,5:7] <- quantile(GEV_Parameters[i,3,],probs=c(0.16,0.50,0.84))
    GEV_Results[i,8:10] <- quantile(GEV_Parameters[i,4,],probs=c(0.16,0.50,0.84))
    }

  Shape <- GEV_Results[,9]  
  Scale <- GEV_Results[,6]
  Location <- GEV_Results[,3]
#+END_SRC

Now we can calculate estimates of maximum magnitudes for arbitrary
time in the future

#+BEGIN_SRC R
  Tau <- c(365000*1:5)
  Q <- 0.975

  Quantiles <- array(0, c(length(Time_Steps),length(Tau)))
  for (Tau_i in 1:length(Tau))
    {
    Quantiles[,Tau_i] <- Location + ((Tau[Tau_i]/(log(1/Q)*Time_Steps))^Shape - 1) * Scale / Shape
    }
#+END_SRC

** Plot results

Let's test how well the reduced data set fulfills the Poisson Process
#+BEGIN_SRC R
  Test_Data <- subset(events, events$Magnitude > M_Min, select=UTC)
  Test_Data$UTC  <- Test_Data$UTC - Test_Data$UTC[1]
  NOE <- length(Test_Data$UTC)
  TL <- Test_Data$UTC[NOE]
  # Generate the equivalent Poissonian data set
  Poisson_Data <- seq(1, TL, by=TL/NOE)
  #  and carry out the Kolmogorov-Smirnov test
  Test_Result <- ks.test(events$UTC,Poisson_Data)
#+END_SRC

Make the actual plots
#+BEGIN_SRC R
  pdf(file="earthquakes_gev.pdf",paper="a4",width=0,height=0,pointsize=10)
  op <- par(mfrow = c(3,2))
  plot(GR,log="y",main="Gutenberg-Richter Plot");grid(lty=2,col=5)
  abline(v=M_Min,col=2)
  My_List <- subset(events, Magnitude > M_Min, select = c(UTC,Magnitude))
  plot(My_List$UTC/My_List$UTC[length(My_List$UTC)],type="l",xlab="Event Number",ylab="Normalised Occurrence Time",main="Poisson Fit");grid(lty=2,col=5)
  abline(0,1/length(My_List$UTC),col=4)
  text(NOE/5,0.8,"p-value:")
  text(NOE/5,0.7,round(Test_Result$p.value,5))
  matplot(GEV_Results[,1],GEV_Results[,8:10],type="l",lty=1,col=c(1,2,1),main="GEV Parameter Estimation",xlab="T Window (days)", ylab="Shape Parameter");grid(lty=2,col=5)
  matplot(GEV_Results[,1],GEV_Results[,5:7],type="l",lty=1,col=c(1,2,1),main="GEV Parameter Estimation",xlab="T Window (days)", ylab="Scale Parameter");grid(lty=2,col=5)
  matplot(GEV_Results[,1],GEV_Results[,2:4],type="l",lty=1,col=c(1,2,1),main="GEV Parameter Estimation",xlab="T Window (days)", ylab="Location Parameter");grid(lty=2,col=5)
  matplot(Time_Steps,Quantiles,ylim=c(6,9),type="l",lty=1,col=1,main="GEV Maximum Magnitude Estimation",xlab="T Window (days)", ylab="0.975 Magnitude Quantile");grid(lty=2,col=5)
  par(op)
  dev.off()
#+END_SRC
